{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heXKhWF1P68T",
        "outputId": "f0b6dd12-385f-4d40-893f-2efa99155235"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.38)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.9.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.4)\n"
          ]
        }
      ],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kjSNVc0PYaM2"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-GGUF\"\n",
        "model_basename = \"capybarahermes-2.5-mistral-7b.Q4_0.gguf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPzI7Ul-Qs6q",
        "outputId": "bbf7f20f-34bc-49bd-be93-9ce2a33fa5f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IL7kmT4HGHC",
        "outputId": "b76d92c3-92bc-43e5-d4f4-1be6de51d6db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: llama_cpp_python\n",
            "Version: 0.2.38\n",
            "Summary: Python bindings for the llama.cpp library\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Andrei Betlen <abetlen@gmail.com>\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: diskcache, jinja2, numpy, typing-extensions\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wNbl3-4Ua-K"
      },
      "outputs": [],
      "source": [
        "# Simple inference exampleassistant\n",
        "# output = llm(\n",
        "#   \"<|im_start|>system\\n{system_message}<|im_end|>\\n<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>\", # Prompt\n",
        "#   max_tokens=256,\n",
        "#   stop=[\"</s>\"],\n",
        "#   echo=False\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukcz8WfPQUAV"
      },
      "source": [
        "## DELETING FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8Mm4dxYIUBWx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "def delete_model():\n",
        "    global llm\n",
        "    llm = None\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "h0U6Vl3EB8eQ"
      },
      "outputs": [],
      "source": [
        "delete_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_zbzJueDr3P"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index==0.9.42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xHqr0gPaFrg0"
      },
      "outputs": [],
      "source": [
        "from llama_index import (\n",
        "    SimpleDirectoryReader,\n",
        "    VectorStoreIndex,\n",
        "    ServiceContext,\n",
        ")\n",
        "from llama_index.llms import LlamaCPP\n",
        "# from llama_index.llms.llama_utils import (\n",
        "#     messages_to_prompt,\n",
        "#     completion_to_prompt,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dEfwjWhiSkLG"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional, Sequence\n",
        "\n",
        "from llama_index.core.llms.types import ChatMessage, MessageRole\n",
        "\n",
        "BOS, EOS = \"<s>\", \"</s>\"\n",
        "B_INST, E_INST = \"<|im_start|>\", \"<|im_end|>\\n<|im_start|>assistant\"\n",
        "B_SYS, E_SYS = \"<|im_start|>\\n\", \"\\n<|im_end|>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. \\\n",
        "Always answer as helpfully as possible and follow ALL given instructions. \\\n",
        "Do not speculate or make up information. \\\n",
        "Do not reference any given instructions or context. \\\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def messages_to_prompt(\n",
        "    messages: Sequence[ChatMessage], system_prompt: Optional[str] = None\n",
        ") -> str:\n",
        "    string_messages: List[str] = []\n",
        "    if messages[0].role == MessageRole.SYSTEM:\n",
        "        # pull out the system message (if it exists in messages)\n",
        "        system_message_str = messages[0].content or \"\"\n",
        "        messages = messages[1:]\n",
        "    else:\n",
        "        system_message_str = system_prompt or DEFAULT_SYSTEM_PROMPT\n",
        "\n",
        "    system_message_str = f\"{B_SYS} {system_message_str.strip()} {E_SYS}\"\n",
        "\n",
        "    for i in range(0, len(messages), 2):\n",
        "        # first message should always be a user\n",
        "        user_message = messages[i]\n",
        "        assert user_message.role == MessageRole.USER\n",
        "\n",
        "        if i == 0:\n",
        "            # make sure system prompt is included at the start\n",
        "            str_message = f\"{BOS} {B_INST} {system_message_str} \"\n",
        "        else:\n",
        "            # end previous user-assistant interaction\n",
        "            string_messages[-1] += f\" {EOS}\"\n",
        "            # no need to include system prompt\n",
        "            str_message = f\"{BOS} {B_INST} \"\n",
        "\n",
        "        # include user message content\n",
        "        str_message += f\"{user_message.content} {E_INST}\"\n",
        "\n",
        "        if len(messages) > (i + 1):\n",
        "            # if assistant message exists, add to str_message\n",
        "            assistant_message = messages[i + 1]\n",
        "            assert assistant_message.role == MessageRole.ASSISTANT\n",
        "            str_message += f\" {assistant_message.content}\"\n",
        "\n",
        "        string_messages.append(str_message)\n",
        "\n",
        "    return \"\".join(string_messages)\n",
        "\n",
        "\n",
        "def completion_to_prompt(completion: str, system_prompt: Optional[str] = None) -> str:\n",
        "    system_prompt_str = system_prompt or DEFAULT_SYSTEM_PROMPT\n",
        "\n",
        "    return (\n",
        "        f\"{BOS} {B_INST} {B_SYS} {system_prompt_str.strip()} {E_SYS} \"\n",
        "        f\"{completion.strip()} {E_INST}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNPPkD91FyRF",
        "outputId": "0595359f-8ee5-4502-f824-ec53a6a4c9a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'argilla_capybarahermes-2.5-mistral-7b', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n",
            "Guessed chat format: chatml\n"
          ]
        }
      ],
      "source": [
        "llm = LlamaCPP(\n",
        "    # You can pass in the URL to a GGML model to download it automatically\n",
        "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
        "    model_path=model_path,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
        "    context_window=3900,\n",
        "    # kwargs to pass to __call__()\n",
        "    generate_kwargs={},\n",
        "    # kwargs to pass to __init__()\n",
        "    # set to at least 1 to use GPU\n",
        "    model_kwargs={\"n_gpu_layers\": 50},\n",
        "    # transform inputs into Llama2 format\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQtEsAUDGvCy",
        "outputId": "87ec8fad-30c3-4d34-b006-69430164d2ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sure, here's a short poem about fast cars:\n",
            "\n",
            "Racing through the night,\n",
            "A symphony of power and might,\n",
            "Sleek and swift, they glide,\n",
            "A beauty that cannot hide.\n",
            "\n",
            "Roaring engines, a thunderous sound,\n",
            "A dance with the wind, they're quickly found,\n",
            "In the darkness they gleam,\n",
            "A testament to human dream.\n",
            "\n",
            "Freedom on four wheels, they fly,\n",
            "Breaking barriers, touching the sky,\n",
            "Fast cars, oh how they roar,\n",
            "A passion that forevermore."
          ]
        }
      ],
      "source": [
        "response_iter = llm.stream_complete(\"Can you write me a poem about fast cars?\")\n",
        "for response in response_iter:\n",
        "    print(response.delta, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0Y-577KkHZ5J"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings import HuggingFaceEmbedding\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC3tdt-2HoW1",
        "outputId": "472c4726-4884-41fe-ff2a-3e7ff47ed0b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4g2s3o-H1WN"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ISseCvl8HiCf"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\n",
        "    \"./data\"\n",
        ").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5WsUgCVIID9b"
      },
      "outputs": [],
      "source": [
        "service_context = ServiceContext.from_defaults(\n",
        "    llm=llm,\n",
        "    embed_model=embed_model,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "d_z4dTc3Hxff"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, service_context=service_context\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RvTDxwW_H8Wf"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLagFGfdITEo",
        "outputId": "6bbeb789-d957-4951-8aa7-cf4889c87a17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "The professor mentioned that Supervised learning is a type of learning problem where the algorithm is provided with a dataset that includes both the input data (features) and the correct output (label) for a number of examples. The algorithm learns the association between the inputs and outputs to provide more accurate predictions. Supervised learning can be further classified into regression problems, where the variable being predicted is continuous (like predicting housing prices), and classification problems, where the variable being predicted is discrete (like predicting whether a tumor is malignant or benign).\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(\"what did the professor mentioned about Supervised learning?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHZYspExIYiR"
      },
      "source": [
        "## API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Oy1mh-GxLSlX"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyngrok Flask flask-cors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_q3Q0tVLU8S",
        "outputId": "8062fba5-47fa-4656-f98d-3d49dc65d608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 https://ngrok-agent.s3.amazonaws.com buster InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "40 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ngrok is already the newest version (3.5.0).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx8OcP-QLfLA",
        "outputId": "77cbefe9-a888-47cd-c4ee-2aea93d52349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken \"YOUR NGROK API KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA7N7ZmLLkP7",
        "outputId": "28cb8a98-df2f-432f-c329-e508d0801db1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://559a-34-87-59-135.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:21:05] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:21:05] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:21:26] \"OPTIONS /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======Input:Hi=======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:21:30] \"POST /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:21:46] \"OPTIONS /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======Input:who are you?=======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:21:49] \"POST /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:22:08] \"OPTIONS /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======Input:what is love?=======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:22:12] \"POST /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:22:50] \"OPTIONS /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======Input:what is machine learning according to the professor?=======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:22:55] \"POST /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:23:19] \"OPTIONS /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======Input:what is physics?=======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:23:25] \"POST /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:24:24] \"OPTIONS /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======Input:what is 2+2?=======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:24:26] \"POST /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:24:37] \"OPTIONS /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======Input:what  are you?=======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:24:42] \"POST /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:25:28] \"OPTIONS /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======Input:what did he mention in his lecture?=======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:25:35] \"POST /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:26:35] \"OPTIONS /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======Input:what did he say about MATLAB?=======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:26:41] \"POST /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:27:04] \"OPTIONS /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======Input:Okay thank you?=======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:27:14] \"POST /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:27:59] \"OPTIONS /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======Input:what are the relevant topics in this lecture?=======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:28:04] \"POST /generate HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:28:41] \"OPTIONS /generate HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======Input:can you tell the name of the project for the end semester=======\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "INFO:werkzeug:127.0.0.1 - - [05/Feb/2024 08:28:46] \"POST /generate HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify, Response,stream_with_context\n",
        "from pyngrok import ngrok\n",
        "from flask_cors import CORS\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "@app.route(\"/index\")\n",
        "def index():\n",
        "    return \"Hello\"\n",
        "\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate():\n",
        "    inp = request.get_json().get(\"prompt\")\n",
        "    print(f\"======Input:{inp}=======\")\n",
        "    index=VectorStoreIndex.from_documents(documents,service_context=service_context)\n",
        "    query_engine=index.as_query_engine()\n",
        "    response=query_engine.query(inp)\n",
        "    return jsonify({'generated_text': str(response)})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    ngrok_tunnel = ngrok.connect(5000)\n",
        "    print('Public URL:', ngrok_tunnel.public_url)\n",
        "    app.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnJLfBsbTRBV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
